{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer: The set of data we can manipulate in our model. Kind of like the X value.\n",
    "### Hidden Layer: A series of behind-the-scenes functions that affect a neural network\n",
    "### Output Layer: The end-result we want from the neural network. Kind of like the Y value.\n",
    "### Neuron: A type of a neural network\n",
    "### Weight: The \"ingredient\" that forms the hidden layer. It changes the more times the neural network runs.\n",
    "### Activation Function: The function that calculates whether something will happen within a hidden layer\n",
    "### Node Map: A diagram of different types of neural networks\n",
    "### Perceptron: The simplest neural network, which takes in input layers and outputs and output layer. No hidden layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "#### Your Answer Here"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Take input -> Apply input to the hidden layer, taking into account a given weight, bias, and activation function that runs from those three numbers -> Get output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [1,1,1,0]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x1</th>\n      <th>x2</th>\n      <th>y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   x1  x2  y\n0   0   0  1\n1   1   0  1\n2   0   1  1\n3   1   1  0"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "inputs = np.array([\n",
    "    [0,0],\n",
    "    [1,0],\n",
    "    [0,1],\n",
    "    [1,1]\n",
    "])\n",
    "\n",
    "correct_outputs = [[1], [1], [1], [0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "\n",
    "# Sigmoid activation function\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sx = sigmoid(x)\n",
    "    return sx * (1-sx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = 2 * np.random.random((2,1)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.04565033],\n       [0.72622549]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.        ],\n       [0.04565033],\n       [0.72622549],\n       [0.77187582]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_sum = np.dot(inputs, weights)\n",
    "weighted_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.5       ],\n       [0.5114106 ],\n       [0.67397644],\n       [0.68392653]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activated_output = sigmoid(weighted_sum)\n",
    "activated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.5       ],\n       [ 0.4885894 ],\n       [ 0.32602356],\n       [-0.68392653]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = correct_outputs - activated_output\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.11750186],\n       [ 0.11449639],\n       [ 0.07290817],\n       [-0.15244864]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted = error * sigmoid_derivative(activated_output)\n",
    "adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.00769808],\n       [0.64668502]])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights += np.dot(inputs.T, adjusted)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Weights after training\n[[-3.05311332e-16]\n [ 1.24900090e-16]]\nOutput after training\n[[0.5]\n [0.5]\n [0.5]\n [0.5]]\n"
    }
   ],
   "source": [
    "for iteration in range(10000):\n",
    "    weighted_sum = np.dot(inputs, weights)\n",
    "\n",
    "    activated_output = sigmoid(weighted_sum)\n",
    "\n",
    "    error = correct_outputs - activated_output\n",
    "\n",
    "    adjustments = error * sigmoid_derivative(activated_output)\n",
    "\n",
    "    weights += np.dot(inputs.T, adjustments)\n",
    "\n",
    "print(\"Weights after training\")\n",
    "print(weights)\n",
    "\n",
    "print(\"Output after training\")\n",
    "print(activated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xf7sdqVs0s4x"
   },
   "source": [
    "## Implement your own Perceptron Class and use it to classify a binary dataset: \n",
    "- [The Pima Indians Diabetes dataset](https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv) \n",
    "\n",
    "You may need to search for other's implementations in order to get inspiration for your own. There are *lots* of perceptron implementations on the internet with varying levels of sophistication and complexity. Whatever your approach, make sure you understand **every** line of your implementation and what its purpose is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pregnancies</th>\n      <th>Glucose</th>\n      <th>BloodPressure</th>\n      <th>SkinThickness</th>\n      <th>Insulin</th>\n      <th>BMI</th>\n      <th>DiabetesPedigreeFunction</th>\n      <th>Age</th>\n      <th>Outcome</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>148</td>\n      <td>72</td>\n      <td>35</td>\n      <td>0</td>\n      <td>33.6</td>\n      <td>0.627</td>\n      <td>50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>85</td>\n      <td>66</td>\n      <td>29</td>\n      <td>0</td>\n      <td>26.6</td>\n      <td>0.351</td>\n      <td>31</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8</td>\n      <td>183</td>\n      <td>64</td>\n      <td>0</td>\n      <td>0</td>\n      <td>23.3</td>\n      <td>0.672</td>\n      <td>32</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>89</td>\n      <td>66</td>\n      <td>23</td>\n      <td>94</td>\n      <td>28.1</td>\n      <td>0.167</td>\n      <td>21</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>137</td>\n      <td>40</td>\n      <td>35</td>\n      <td>168</td>\n      <td>43.1</td>\n      <td>2.288</td>\n      <td>33</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n0            6      148             72             35        0  33.6   \n1            1       85             66             29        0  26.6   \n2            8      183             64              0        0  23.3   \n3            1       89             66             23       94  28.1   \n4            0      137             40             35      168  43.1   \n\n   DiabetesPedigreeFunction  Age  Outcome  \n0                     0.627   50        1  \n1                     0.351   31        0  \n2                     0.672   32        1  \n3                     0.167   21        0  \n4                     2.288   33        1  "
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['Pregnancies',\n 'Glucose',\n 'BloodPressure',\n 'SkinThickness',\n 'Insulin',\n 'BMI',\n 'DiabetesPedigreeFunction',\n 'Age']"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "feats = list(diabetes)[:-1]\n",
    "\n",
    "feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[6.000e+00, 1.480e+02, 7.200e+01, 3.500e+01, 0.000e+00, 3.360e+01,\n        6.270e-01],\n       [1.000e+00, 8.500e+01, 6.600e+01, 2.900e+01, 0.000e+00, 2.660e+01,\n        3.510e-01],\n       [8.000e+00, 1.830e+02, 6.400e+01, 0.000e+00, 0.000e+00, 2.330e+01,\n        6.720e-01],\n       [1.000e+00, 8.900e+01, 6.600e+01, 2.300e+01, 9.400e+01, 2.810e+01,\n        1.670e-01],\n       [0.000e+00, 1.370e+02, 4.000e+01, 3.500e+01, 1.680e+02, 4.310e+01,\n        2.288e+00],\n       [5.000e+00, 1.160e+02, 7.400e+01, 0.000e+00, 0.000e+00, 2.560e+01,\n        2.010e-01],\n       [3.000e+00, 7.800e+01, 5.000e+01, 3.200e+01, 8.800e+01, 3.100e+01,\n        2.480e-01],\n       [1.000e+01, 1.150e+02, 0.000e+00, 0.000e+00, 0.000e+00, 3.530e+01,\n        1.340e-01],\n       [2.000e+00, 1.970e+02, 7.000e+01, 4.500e+01, 5.430e+02, 3.050e+01,\n        1.580e-01],\n       [8.000e+00, 1.250e+02, 9.600e+01, 0.000e+00, 0.000e+00, 0.000e+00,\n        2.320e-01],\n       [4.000e+00, 1.100e+02, 9.200e+01, 0.000e+00, 0.000e+00, 3.760e+01,\n        1.910e-01],\n       [1.000e+01, 1.680e+02, 7.400e+01, 0.000e+00, 0.000e+00, 3.800e+01,\n        5.370e-01],\n       [1.000e+01, 1.390e+02, 8.000e+01, 0.000e+00, 0.000e+00, 2.710e+01,\n        1.441e+00],\n       [1.000e+00, 1.890e+02, 6.000e+01, 2.300e+01, 8.460e+02, 3.010e+01,\n        3.980e-01],\n       [5.000e+00, 1.660e+02, 7.200e+01, 1.900e+01, 1.750e+02, 2.580e+01,\n        5.870e-01],\n       [7.000e+00, 1.000e+02, 0.000e+00, 0.000e+00, 0.000e+00, 3.000e+01,\n        4.840e-01],\n       [0.000e+00, 1.180e+02, 8.400e+01, 4.700e+01, 2.300e+02, 4.580e+01,\n        5.510e-01],\n       [7.000e+00, 1.070e+02, 7.400e+01, 0.000e+00, 0.000e+00, 2.960e+01,\n        2.540e-01],\n       [1.000e+00, 1.030e+02, 3.000e+01, 3.800e+01, 8.300e+01, 4.330e+01,\n        1.830e-01],\n       [1.000e+00, 1.150e+02, 7.000e+01, 3.000e+01, 9.600e+01, 3.460e+01,\n        5.290e-01],\n       [3.000e+00, 1.260e+02, 8.800e+01, 4.100e+01, 2.350e+02, 3.930e+01,\n        7.040e-01],\n       [8.000e+00, 9.900e+01, 8.400e+01, 0.000e+00, 0.000e+00, 3.540e+01,\n        3.880e-01],\n       [7.000e+00, 1.960e+02, 9.000e+01, 0.000e+00, 0.000e+00, 3.980e+01,\n        4.510e-01],\n       [9.000e+00, 1.190e+02, 8.000e+01, 3.500e+01, 0.000e+00, 2.900e+01,\n        2.630e-01],\n       [1.100e+01, 1.430e+02, 9.400e+01, 3.300e+01, 1.460e+02, 3.660e+01,\n        2.540e-01],\n       [1.000e+01, 1.250e+02, 7.000e+01, 2.600e+01, 1.150e+02, 3.110e+01,\n        2.050e-01],\n       [7.000e+00, 1.470e+02, 7.600e+01, 0.000e+00, 0.000e+00, 3.940e+01,\n        2.570e-01],\n       [1.000e+00, 9.700e+01, 6.600e+01, 1.500e+01, 1.400e+02, 2.320e+01,\n        4.870e-01],\n       [1.300e+01, 1.450e+02, 8.200e+01, 1.900e+01, 1.100e+02, 2.220e+01,\n        2.450e-01],\n       [5.000e+00, 1.170e+02, 9.200e+01, 0.000e+00, 0.000e+00, 3.410e+01,\n        3.370e-01],\n       [5.000e+00, 1.090e+02, 7.500e+01, 2.600e+01, 0.000e+00, 3.600e+01,\n        5.460e-01],\n       [3.000e+00, 1.580e+02, 7.600e+01, 3.600e+01, 2.450e+02, 3.160e+01,\n        8.510e-01],\n       [3.000e+00, 8.800e+01, 5.800e+01, 1.100e+01, 5.400e+01, 2.480e+01,\n        2.670e-01],\n       [6.000e+00, 9.200e+01, 9.200e+01, 0.000e+00, 0.000e+00, 1.990e+01,\n        1.880e-01],\n       [1.000e+01, 1.220e+02, 7.800e+01, 3.100e+01, 0.000e+00, 2.760e+01,\n        5.120e-01],\n       [4.000e+00, 1.030e+02, 6.000e+01, 3.300e+01, 1.920e+02, 2.400e+01,\n        9.660e-01],\n       [1.100e+01, 1.380e+02, 7.600e+01, 0.000e+00, 0.000e+00, 3.320e+01,\n        4.200e-01],\n       [9.000e+00, 1.020e+02, 7.600e+01, 3.700e+01, 0.000e+00, 3.290e+01,\n        6.650e-01],\n       [2.000e+00, 9.000e+01, 6.800e+01, 4.200e+01, 0.000e+00, 3.820e+01,\n        5.030e-01],\n       [4.000e+00, 1.110e+02, 7.200e+01, 4.700e+01, 2.070e+02, 3.710e+01,\n        1.390e+00],\n       [3.000e+00, 1.800e+02, 6.400e+01, 2.500e+01, 7.000e+01, 3.400e+01,\n        2.710e-01],\n       [7.000e+00, 1.330e+02, 8.400e+01, 0.000e+00, 0.000e+00, 4.020e+01,\n        6.960e-01],\n       [7.000e+00, 1.060e+02, 9.200e+01, 1.800e+01, 0.000e+00, 2.270e+01,\n        2.350e-01],\n       [9.000e+00, 1.710e+02, 1.100e+02, 2.400e+01, 2.400e+02, 4.540e+01,\n        7.210e-01],\n       [7.000e+00, 1.590e+02, 6.400e+01, 0.000e+00, 0.000e+00, 2.740e+01,\n        2.940e-01],\n       [0.000e+00, 1.800e+02, 6.600e+01, 3.900e+01, 0.000e+00, 4.200e+01,\n        1.893e+00],\n       [1.000e+00, 1.460e+02, 5.600e+01, 0.000e+00, 0.000e+00, 2.970e+01,\n        5.640e-01],\n       [2.000e+00, 7.100e+01, 7.000e+01, 2.700e+01, 0.000e+00, 2.800e+01,\n        5.860e-01],\n       [7.000e+00, 1.030e+02, 6.600e+01, 3.200e+01, 0.000e+00, 3.910e+01,\n        3.440e-01],\n       [7.000e+00, 1.050e+02, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n        3.050e-01],\n       [1.000e+00, 1.030e+02, 8.000e+01, 1.100e+01, 8.200e+01, 1.940e+01,\n        4.910e-01],\n       [1.000e+00, 1.010e+02, 5.000e+01, 1.500e+01, 3.600e+01, 2.420e+01,\n        5.260e-01],\n       [5.000e+00, 8.800e+01, 6.600e+01, 2.100e+01, 2.300e+01, 2.440e+01,\n        3.420e-01],\n       [8.000e+00, 1.760e+02, 9.000e+01, 3.400e+01, 3.000e+02, 3.370e+01,\n        4.670e-01],\n       [7.000e+00, 1.500e+02, 6.600e+01, 4.200e+01, 3.420e+02, 3.470e+01,\n        7.180e-01],\n       [1.000e+00, 7.300e+01, 5.000e+01, 1.000e+01, 0.000e+00, 2.300e+01,\n        2.480e-01],\n       [7.000e+00, 1.870e+02, 6.800e+01, 3.900e+01, 3.040e+02, 3.770e+01,\n        2.540e-01],\n       [0.000e+00, 1.000e+02, 8.800e+01, 6.000e+01, 1.100e+02, 4.680e+01,\n        9.620e-01],\n       [0.000e+00, 1.460e+02, 8.200e+01, 0.000e+00, 0.000e+00, 4.050e+01,\n        1.781e+00],\n       [0.000e+00, 1.050e+02, 6.400e+01, 4.100e+01, 1.420e+02, 4.150e+01,\n        1.730e-01],\n       [2.000e+00, 8.400e+01, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n        3.040e-01],\n       [8.000e+00, 1.330e+02, 7.200e+01, 0.000e+00, 0.000e+00, 3.290e+01,\n        2.700e-01],\n       [5.000e+00, 4.400e+01, 6.200e+01, 0.000e+00, 0.000e+00, 2.500e+01,\n        5.870e-01],\n       [2.000e+00, 1.410e+02, 5.800e+01, 3.400e+01, 1.280e+02, 2.540e+01,\n        6.990e-01],\n       [7.000e+00, 1.140e+02, 6.600e+01, 0.000e+00, 0.000e+00, 3.280e+01,\n        2.580e-01],\n       [5.000e+00, 9.900e+01, 7.400e+01, 2.700e+01, 0.000e+00, 2.900e+01,\n        2.030e-01],\n       [0.000e+00, 1.090e+02, 8.800e+01, 3.000e+01, 0.000e+00, 3.250e+01,\n        8.550e-01],\n       [2.000e+00, 1.090e+02, 9.200e+01, 0.000e+00, 0.000e+00, 4.270e+01,\n        8.450e-01],\n       [1.000e+00, 9.500e+01, 6.600e+01, 1.300e+01, 3.800e+01, 1.960e+01,\n        3.340e-01],\n       [4.000e+00, 1.460e+02, 8.500e+01, 2.700e+01, 1.000e+02, 2.890e+01,\n        1.890e-01],\n       [2.000e+00, 1.000e+02, 6.600e+01, 2.000e+01, 9.000e+01, 3.290e+01,\n        8.670e-01],\n       [5.000e+00, 1.390e+02, 6.400e+01, 3.500e+01, 1.400e+02, 2.860e+01,\n        4.110e-01],\n       [1.300e+01, 1.260e+02, 9.000e+01, 0.000e+00, 0.000e+00, 4.340e+01,\n        5.830e-01],\n       [4.000e+00, 1.290e+02, 8.600e+01, 2.000e+01, 2.700e+02, 3.510e+01,\n        2.310e-01],\n       [1.000e+00, 7.900e+01, 7.500e+01, 3.000e+01, 0.000e+00, 3.200e+01,\n        3.960e-01],\n       [1.000e+00, 0.000e+00, 4.800e+01, 2.000e+01, 0.000e+00, 2.470e+01,\n        1.400e-01],\n       [7.000e+00, 6.200e+01, 7.800e+01, 0.000e+00, 0.000e+00, 3.260e+01,\n        3.910e-01],\n       [5.000e+00, 9.500e+01, 7.200e+01, 3.300e+01, 0.000e+00, 3.770e+01,\n        3.700e-01],\n       [0.000e+00, 1.310e+02, 0.000e+00, 0.000e+00, 0.000e+00, 4.320e+01,\n        2.700e-01],\n       [2.000e+00, 1.120e+02, 6.600e+01, 2.200e+01, 0.000e+00, 2.500e+01,\n        3.070e-01],\n       [3.000e+00, 1.130e+02, 4.400e+01, 1.300e+01, 0.000e+00, 2.240e+01,\n        1.400e-01],\n       [2.000e+00, 7.400e+01, 0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00,\n        1.020e-01],\n       [7.000e+00, 8.300e+01, 7.800e+01, 2.600e+01, 7.100e+01, 2.930e+01,\n        7.670e-01],\n       [0.000e+00, 1.010e+02, 6.500e+01, 2.800e+01, 0.000e+00, 2.460e+01,\n        2.370e-01],\n       [5.000e+00, 1.370e+02, 1.080e+02, 0.000e+00, 0.000e+00, 4.880e+01,\n        2.270e-01],\n       [2.000e+00, 1.100e+02, 7.400e+01, 2.900e+01, 1.250e+02, 3.240e+01,\n        6.980e-01],\n       [1.300e+01, 1.060e+02, 7.200e+01, 5.400e+01, 0.000e+00, 3.660e+01,\n        1.780e-01],\n       [2.000e+00, 1.000e+02, 6.800e+01, 2.500e+01, 7.100e+01, 3.850e+01,\n        3.240e-01],\n       [1.500e+01, 1.360e+02, 7.000e+01, 3.200e+01, 1.100e+02, 3.710e+01,\n        1.530e-01],\n       [1.000e+00, 1.070e+02, 6.800e+01, 1.900e+01, 0.000e+00, 2.650e+01,\n        1.650e-01],\n       [1.000e+00, 8.000e+01, 5.500e+01, 0.000e+00, 0.000e+00, 1.910e+01,\n        2.580e-01],\n       [4.000e+00, 1.230e+02, 8.000e+01, 1.500e+01, 1.760e+02, 3.200e+01,\n        4.430e-01],\n       [7.000e+00, 8.100e+01, 7.800e+01, 4.000e+01, 4.800e+01, 4.670e+01,\n        2.610e-01],\n       [4.000e+00, 1.340e+02, 7.200e+01, 0.000e+00, 0.000e+00, 2.380e+01,\n        2.770e-01],\n       [2.000e+00, 1.420e+02, 8.200e+01, 1.800e+01, 6.400e+01, 2.470e+01,\n        7.610e-01],\n       [6.000e+00, 1.440e+02, 7.200e+01, 2.700e+01, 2.280e+02, 3.390e+01,\n        2.550e-01],\n       [2.000e+00, 9.200e+01, 6.200e+01, 2.800e+01, 0.000e+00, 3.160e+01,\n        1.300e-01],\n       [1.000e+00, 7.100e+01, 4.800e+01, 1.800e+01, 7.600e+01, 2.040e+01,\n        3.230e-01],\n       [6.000e+00, 9.300e+01, 5.000e+01, 3.000e+01, 6.400e+01, 2.870e+01,\n        3.560e-01],\n       [1.000e+00, 1.220e+02, 9.000e+01, 5.100e+01, 2.200e+02, 4.970e+01,\n        3.250e-01]])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = diabetes.iloc[0:100,0:7].values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n       1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n       0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = diabetes.iloc[0:100,8].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Update this Class #####\n",
    "\n",
    "class Perceptron(object):\n",
    "    \n",
    "    def __init__(self, niter = 10):\n",
    "        self.niter = niter\n",
    "    \n",
    "    def __sigmoid(self, x):\n",
    "        return None\n",
    "    \n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data\n",
    "        X : Training vectors, X.shape : [#samples, #features]\n",
    "        y : Target values, y.shape : [#samples]\n",
    "        \"\"\"\n",
    "\n",
    "        # Randomly Initialize Weights\n",
    "        weights = 2 * np.random.random((2,1)) - 1\n",
    "\n",
    "        for i in range(self.niter):\n",
    "            # Weighted sum of inputs / weights\n",
    "            weighted_sum = np.dot(inputs, weights)\n",
    "\n",
    "            # Activate!\n",
    "            activated_output = sigmoid(weighted_sum)\n",
    "\n",
    "            # Cac error\n",
    "            error = correct_outputs - activated_output\n",
    "            adjusted = error * sigmoid_derivative(activated_output)\n",
    "\n",
    "            # Update the Weights\n",
    "            weights += np.dot(inputs.T, adjustments)\n",
    "        \n",
    "        print(\"Weights after training\")\n",
    "        print(weights)\n",
    "\n",
    "        print(\"Output after training\")\n",
    "        print(activated_output)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'niter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-9596f52d997e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mPerceptron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-84e60b690083>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mniter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0;31m# Weighted sum of inputs / weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mweighted_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'niter'"
     ]
    }
   ],
   "source": [
    "Perceptron.fit(10000,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}